{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VANISHING GRADIENT and EXPLODING GRADIENT \n",
    "Even if in principle it's possible to do model any function with an NN with only one layer, however it is not convenient! You could require many many nodes. Usually it's better to use more layers. It is not clear what each layer does, but it is thought that deeper layers are associated to detailed features of the input data. For instance about visual pattern recognition, the neurons in the first layer might learn to recognize edges, the neurons in the second layer could learn to recognize more complex shapes, say triangle or rectangles, built up from edges. The third layer would then recognize still more complex shapes. And so on.\n",
    "\n",
    "But there's a problem: Deep Neural Network (DNN) can be harder to train, so to really get best results you must deal with hyper-parameter tuning.\n",
    "\n",
    "By observing the learning of a DNN sometimes can be seen that the different layers are learning at vastly different speeds. In particular, when later layers in the network are learning well, early layers often get stuck during training, learning almost nothing at all, or vice versa. This happens because there's an intrinsic instability associated to learning by gradient descent in deep neural networks. \n",
    "\n",
    "Let's see what's the problem:\n",
    "\n",
    "Remember that $\\delta = \\frac{\\partial C}{\\partial z} = \\frac{\\partial C}{\\partial b}$, and it also determines the gradient wrt the weights, so we can consider $\\delta$ as the indicator of speed of each neuron. If we look at this quantity we see a trend: layers closer to the output layer learn faster, from the beginning. This is a picture which shows it considering two layers:\n",
    "\n",
    "<img src=\"speed_learning.png\" width=30% height=30%>\n",
    "This is the trend with 4 hidden layers:\n",
    "<img src=\"speed_learning2.png\" width=30% height=30%>\n",
    "note that the first layer learns 100 times nore slowly than the final one.\n",
    "\n",
    "This is what is called the vanishing gradient problem.\n",
    "\n",
    "When we try to fix the vanishing gradient problem, or when for some reason the weights have a great value, or depending on the activation function used, it could happen that we get the opposite problem: the final hidden layers learn too slowly. This is why the gradient is considered instable.\n",
    "\n",
    "Let's have an intuition of the origin of the vanishing gradient:\n",
    "Consider a super easy Deep Neural Network containing only one node per layer. Let's compute the gradient of C wrt the bias (which as we said represent the speed of learning) for the node in the first hidden layer, simply applying backpropagation:\n",
    "\n",
    "<img src=\"C_b.png\" width=60% height=60%>\n",
    "\n",
    "As you can see at each layer it's added a multiplicative factor: $w*\\sigma'(z)$. By looking the graph of the $\\sigma'(z)$ you can see that it has the maximum when z=0, and the value is 1/4:\n",
    "\n",
    "<img src=\"sigma_der.png\" width=30% height=30%>\n",
    "\n",
    "And since the weights are initialized in order to have small values, it's higly probable that $|w*\\sigma'(z)|<1/4$.\n",
    "If instead we have weights with high values, and the bias values which make z close to zero so that the derivative is close to its maximum, then it happens that we have the exploding gradient problem.\n",
    "\n",
    "My idea: in order to equilibrate the multiplicative factor I use a parameter called \"my\" which multiplies delta at each layer by a number. If I think to have a vanishing problem I set my>1, If I think to have the exploding problem I set my<1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
